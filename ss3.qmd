---
title: 'Part 3: Exploring remotely-stored Zarr datasets using Xarray'
---



## A brief intro to multi- or N-dimensional datasets

Multi- or N-dimensional geospatial data refers to datasets that include multiple layers or dimensions of geographical information. This data goes beyond the simple two-dimensional latitude and longitude (or y and x) coordinates we have seen in the earlier notebooks and often includes additional dimensions such as time, elevation (altitude or depth), and other variables like temperature, humidity, rainfall.

This notebook was designed for a session as part of the UKCEH Summer School. It does not cover all aspects of multi-dimensional data use by the Python scientific communities. Additional resources can be found throughout the notebook.

## Contents
- Loading N-dimensional datasets from object storage
- Using Xarray with Dask to explore N-dimensional datasets
- Plotting N-dimensional datasets with Xarray and matplotlib
- Computing a climatology
- Using shapefiles to "cut-out" areas of N-dimensional datasets

Import packages

```{python}
# ONLY NEEDS TO BE RUN IF USING GOOGLE COLABS
#!pip install s3fs cartopy zarr boto3 netcdf4 rasterio
```

```{python}
import os
import s3fs
import boto3
import xarray as xr
import numpy as np
import cartopy as cp
import geopandas as gpd
from dask.diagnostics import ProgressBar
import matplotlib.pyplot as plt
import matplotlib.animation as animation
import datetime
import zarr
```

In this notebook we will explore how to work with gridded N-dimensional datasets stored remotely. Specifically we will be looking at datasets stored as [Zarr](https://zarr.dev/), which is a format becoming increasingly common in big-data science where datasets need to be stored in remote "object stores" due to their size.

[NetCDF](https://www.unidata.ucar.edu/software/netcdf/) is another very common format for gridded N-dimensional datasets. Whilst we will not explicitly work with NetCDF data here, once data is read into [Xarray](https://docs.xarray.dev/en/stable/) it behaves in the same way and the same commands can be used.

### Access and explore the datasets in object store buckets (using FSSpec and Xarray)

We will be working with an hourly gridded rainfall dataset - [UKCEH GEAR](https://catalogue.ceh.ac.uk/documents/fc9423d6-3d54-467f-bb2b-fc7357a3941f)

For publically available data such as this, we do not need any credentials, and instead pass ```anon=True``` which means 'access the data as an anonymous user'.

We will use the [FSSpec](https://filesystem-spec.readthedocs.io/en/latest/) package, which basically has the ability to take any data stored on any storage system and make it look as if the data is on an ordinary disk to the rest of your code. The idea is that you run this bit of code once and then it gets out the way and lets you continue your coding as if the data was still on disk and not in a remote object store in the cloud.

To make it work we provide ```anon=True``` and the ```endpoint_url```, but also the path to the specific dataset we want to use:

**Note:** There are two ways of doing this, depending on which version of the Zarr package is installed in your python environment. To check, run:

```{python}
zarr.__version__
```

```{python}
# zarr v2 method
#zstore = fsspec.get_mapper('s3://gearhrly/gearhrly_fulloutput_yearly_100km_chunks.zarr',
#                           anon=True,
#                           endpoint_url="https://fdri-o.s3-ext.jc.rl.ac.uk")

# zarr v3 method
fs = s3fs.S3FileSystem(anon=True, asynchronous=True, endpoint_url="https://fdri-o.s3-ext.jc.rl.ac.uk")
zstore = zarr.storage.FsspecStore(fs, path="gearhrly/gearhrly_15day_100km_chunks.zarr")
```

We then pass the created file-system-like object to Xarray, and from here on in Xarray behaves as if the data were stored locally on disk, and we can largely forget about the fact that it's actually stored remotely in the cloud.

```{python}
ds = xr.open_zarr(zstore, consolidated=True)
```

Next let's explore the dataset a bit.

```{python}
ds
```

The variables in the dataset are split into those that describe the coordinates, and those of the main data. We can see the three main data variables: 'min_dist', 'rainfall_amount' and 'stat_disag', you can click the page icon at the end of each variable's row to find out a little more about each.

The names in brackets next to the variable names (the second column of information) show you the dimensions that each variable is on. Here, all our data variables are on a 3D grid of time, y and x.

You can see in the Coordinates section that we have other coordinates besides those for the time, y and x dimensions. The lat and lon variables tell you the latitude and longitude conversion for each x,y gridpoint and the 'xxx_bnds' variables tell you the extent/valid range of each datapoint. So for example a gridpoint of (6000,6000) on a grid with a resolution of (1000,1000) would have extents/boundaries of (5500,6500) for both x and y, describing the extent of the gridbox this gridpoint represents. The same concept can be extended to the time-dimension too.

```{python}
#| scrolled: true
from PIL import Image
fs_img = s3fs.S3FileSystem(anon=True, endpoint_url="https://fdri-o.s3-ext.jc.rl.ac.uk")
display(Image.open(fs_img.open('s3://example-data/Scratch-1.png')))
```

More information about the dataset is available by clicking on 'Attributes' to expand this section

All Zarr datasets that follow the [CF-Conventions](https://cfconventions.org/) (a format standard for NetCDF, and Zarr by extension, originally created for climate-model data but slowly being expanded out to cover more disciplines in the geosciences) should look similar to this one.

Variables are selected using ```datasetname['variablename']``` syntax

The ```.data``` attribute pulls out the underlying data array of the specified variable, which in the case of Zarr will be a Dask array. Dask arrays, as opposed to standard Numpy arrays you might be used to working with, are 'chunked' into little parcels of data. It's a feature of the format that makes it most suitable for storage on the cloud.

```{python}
ds['rainfall_amount'].data
```

- **Bytes** shows you the overall size of the array and indivdual chunk
- **Shape** shows you the dimension sizes of the array and chunk. This dataset has 236688 time steps!
- **Dask graph** tells you how many tasks (68432 here) would be needed to load the actual data into memory (see below section on Lazy Loading)
- **Data type** tells you what the data inside the array consists of. Here it is a Numpy array containing 64-bit (8-byte) floats (floating-point numbers), which is typical for numeric data.

### Computations and visualisations with Xarray and Dask

#### An introduction to Lazy Loading

Before we go any further it is very important to know that Xarray loads data *lazily*. This means that the actual data is never loaded or computed until it absolutely has to be. So you may run some code that computes the mean of the dataset (e.g. ```dmean = ds['variable'].mean(dim='time')```), but the computation will only actually be carried out when you want to *see* the result of this calculation, e.g. through a plot or print statement, or saving to disk.

[Dask](https://www.dask.org/) also takes this a step further. Dask is a library designed to automatically parallelise computations; parallelisation is essential when dealing with chunked data such as Zarr on the cloud. Dask sits in the background working out how best to parallelise your computations, then whenever Xarray actually triggers a compute, Dask will automatically kick in and process the computation in parallel. However, with Dask, the *output* of the computation will not persist in memory, even if you save the output to a variable (e.g. ```dmean = ds['variable'].mean(dim='time')```). If you run the code again, Dask will *recompute* everything from scratch, the actual data is not stored in the variable ```dmean```, rather the Dask-instructions for computing it. This can make code very slow if not handled correctly. The best way to handle this is to make use of Dask's [```.persist()```](https://distributed.dask.org/en/latest/api.html?highlight=persist#distributed.Client.persist)and [```.compute()```](https://docs.dask.org/en/stable/api.html?highlight=compute#dask.compute) methods. Appending this on to the end of a calculation, such as ```dmean = ds['variable'].mean(dim='time').compute()``` will keep the data in memory.

The main difference between [```persist```](https://distributed.dask.org/en/latest/api.html?highlight=persist#distributed.Client.persist) and [```compute```](https://docs.dask.org/en/stable/api.html?highlight=compute#dask.compute) is that [```persist```](https://distributed.dask.org/en/latest/api.html?highlight=persist#distributed.Client.persist) will allow you to continue coding whilst dask computes the result in the background, whereas [```compute```](https://docs.dask.org/en/stable/api.html?highlight=compute#dask.compute) will wait until the computation is complete before letting you continue coding. I tend to find [```compute```](https://docs.dask.org/en/stable/api.html?highlight=compute#dask.compute) behaves the most intuitively.

Now with that out the way let's get to actually looking at the data!

#### Plot an individual time step

You can index the array and pull out a single timestep like you can any standard array-like object in Python

Xarray also adds a ```plot()``` method you can call to produce a rough-and-ready quick plot of the data you've selected, in this case the 37th timestep of the dataset

```{python}
with ProgressBar():
    ds['rainfall_amount'][36,:,:].plot()
```

#### Customising plots

These plots can be customised to look nicer too. Note that we're making use of Dask's [```compute()```](https://docs.dask.org/en/stable/api.html?highlight=compute#dask.compute) method to save the variable ```plotpoint``` in memory, so that we don't have to rerun the processing needed to extract this point from the cloud whenever we rerun the plotting commands:

```{python}
with ProgressBar():
    plotpoint = ds['rainfall_amount'][36,:,:].compute()
```

```{python}
plotpoint.plot.pcolormesh('lon', 'lat', cmap='Blues', robust=True)
plt.title(r'Rainfall $2^{nd}$ Jan 1990 12:00')
```

Here we have:
- changed the axes to use lon/lat instead of x/y coordinates by specifying the dataset name of the longitude and latitude variables in the plotting command
- changed the colourmap to a nicer one (all pre-built colourmaps can be found at https://matplotlib.org/3.1.0/tutorials/colors/colormaps.html)
- changed the title to better describe the data being plotted
- used Xarray's 'robust' option, which modifies the colourbar to not stretch to the maximum of the data if it is an outlier. This stops a small area of very high values dominating the colourbar and making other variation invisible

But it would also be nice to have some coastlines. For this we have to go beyond what Xarray's built-in plotting can acheive and use the Cartopy package:

```{python}
import cartopy.crs as ccrs # the set of map projections cartopy supports
import cartopy as cp # the full cartopy package
```

Passing the ```projection``` 'key-word argument' to the plotting function tells the plotting library to invoke cartopy to draw the plot using a given map projection. Here we are using the 'OSGB' map projection (a cartesian grid, in other words a flat plane approximation that ignores the curvature of the earth) which a lot of UK hydrological data will be on. For any data that is on a 'lon/lat' grid instead of an 'x/y' grid, the [```ccrs.PlateCarree()```](https://scitools.org.uk/cartopy/docs/v0.15/crs/projections.html#platecarree) projection is a better option.

```{python}
plot1 = plotpoint.plot.pcolormesh(cmap='Blues', robust=True, subplot_kws=dict(projection=ccrs.OSGB())) # create the initial plot
plt.title(r'Rainfall $2^{nd}$ Jan 1990 12:00')
plot1.axes.coastlines() # add coastlines
gl = plot1.axes.gridlines(draw_labels=True, alpha=0.5) # add gridlines. The alpha parameter is the transparency between 0 and 1.
gl.top_labels = False
gl.right_labels = False # remove the top and right gridlines labels to make the plot look nicer
```

## Further Resources

- [Xarray documentation](https://docs.xarray.dev/en/stable/)
- [Xarray "common usage patterns" tutorial](https://tutorial.xarray.dev/intermediate/01-high-level-computation-patterns.html)
- [Dask documentation](https://docs.dask.org/en/stable/index.html)
- [Dask tutorial](https://ncar.github.io/dask-tutorial/notebooks/00-dask-overview.html)
- [FSSpec documentation](https://filesystem-spec.readthedocs.io/en/latest/)
- [Pangeo tutorial gallery](https://gallery.pangeo.io/repos/pangeo-data/pangeo-tutorial-gallery/index.html)
- [Object storage tutorial](https://github.com/NERC-CEH/object_store_tutorial)
- [**2024** Summer School workshop notebook 1](https://github.com/hydro-jules/school/blob/main/HJ-SS_Workshop-4/HJ-SS_Workshop-4.ipynb), focusing more on NetCDF
- [**2024** Summer School workshop notebook 2](https://github.com/hydro-jules/school/blob/main/HJ-SS_Workshop-5/HJ-SS_Workshop-5.ipynb), focusing more on Zarr
